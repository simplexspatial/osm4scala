(window.webpackJsonp=window.webpackJsonp||[]).push([[6],{129:function(e,a,t){"use strict";t.r(a),a.default=t.p+"assets/images/notebook_screenshoot-9fb4d00eb90d069d41a895f8bf170b1f.png"},76:function(e,a,t){"use strict";t.r(a),t.d(a,"frontMatter",(function(){return i})),t.d(a,"metadata",(function(){return s})),t.d(a,"toc",(function(){return o})),t.d(a,"default",(function(){return p}));var n=t(3),r=t(7),l=(t(0),t(90)),i={title:"Spark SQL Connector"},s={unversionedId:"spark-connector",id:"spark-connector",isDocsHomePage:!1,title:"Spark SQL Connector",description:"Overview",source:"@site/docs/spark-connector.mdx",slug:"/spark-connector",permalink:"/osm4scala/docs/spark-connector",editUrl:"https://github.com/simplexspatial/osm4scala/edit/master/website/docs/spark-connector.mdx",version:"current",sidebar:"docs",previous:{title:"Overview",permalink:"/osm4scala/docs/"},next:{title:"Scala library",permalink:"/osm4scala/docs/standalone-scala-library"}},o=[{value:"Overview",id:"overview",children:[]},{value:"Options",id:"options",children:[]},{value:"Schema definition",id:"schema-definition",children:[]},{value:"All in one jar",id:"all-in-one-jar",children:[{value:"Why a fat shaded library?",id:"why-a-fat-shaded-library",children:[]},{value:"Spark Shell",id:"spark-shell",children:[]},{value:"Notebook",id:"notebook",children:[]},{value:"Spark application",id:"spark-application",children:[]},{value:"More Examples",id:"more-examples",children:[]}]},{value:"Plain (non-shaded jar) dependency.",id:"plain-non-shaded-jar-dependency",children:[{value:"Resolving dependency conflicts",id:"resolving-dependency-conflicts",children:[]}]}],c={toc:o};function p(e){var a=e.components,i=Object(r.a)(e,["components"]);return Object(l.b)("wrapper",Object(n.a)({},c,i,{components:a,mdxType:"MDXLayout"}),Object(l.b)("h2",{id:"overview"},"Overview"),Object(l.b)("p",null,"Using ",Object(l.b)("em",{parentName:"p"},"Osm4scala Spark SQL Connector"),", reading OSM Pbf file from ",Object(l.b)("strong",{parentName:"p"},"PySpark"),", ",Object(l.b)("strong",{parentName:"p"},"Spark Scala"),", ",Object(l.b)("strong",{parentName:"p"},"SparkSQL")," or ",Object(l.b)("strong",{parentName:"p"},"SparkR")," is\nso easy as writing ",Object(l.b)("inlineCode",{parentName:"p"},'.read.format("osm.pbf")'),"."),Object(l.b)("p",null,"The current implementation offers:"),Object(l.b)("ul",null,Object(l.b)("li",{parentName:"ul"},"Spark 2 and 3 versions, with Scala 2.11 and 2.12"),Object(l.b)("li",{parentName:"ul"},"Full Spark SQL integration."),Object(l.b)("li",{parentName:"ul"},"Easy schema."),Object(l.b)("li",{parentName:"ul"},"Internal optimizations, like:",Object(l.b)("ul",{parentName:"li"},Object(l.b)("li",{parentName:"ul"},"Transparent parallelism reading multiply pbf files."),Object(l.b)("li",{parentName:"ul"},"File splitting to increase parallelism per pbf file."),Object(l.b)("li",{parentName:"ul"},"Pushdown required columns.")))),Object(l.b)("p",null,"The library is distributed via ",Object(l.b)("a",{parentName:"p",href:"https://mvnrepository.com/artifact/com.acervera.osm4scala"},"Maven Repo")," in two different flavours:"),Object(l.b)("ul",null,Object(l.b)("li",{parentName:"ul"},Object(l.b)("a",{parentName:"li",href:"#all-in-one-jar"},"All in one jar")," to be able to use directly with all dependencies"),Object(l.b)("li",{parentName:"ul"},"As ",Object(l.b)("a",{parentName:"li",href:"#plain-non-shaded-jar-dependency"},"plain scala dependency"),".")),Object(l.b)("h2",{id:"options"},"Options"),Object(l.b)("p",null,"This is the list of options available when creating a dataframe:"),Object(l.b)("table",null,Object(l.b)("thead",{parentName:"table"},Object(l.b)("tr",{parentName:"thead"},Object(l.b)("th",{parentName:"tr",align:"center"},"Option"),Object(l.b)("th",{parentName:"tr",align:"center"},"default"),Object(l.b)("th",{parentName:"tr",align:"center"},"possible values"),Object(l.b)("th",{parentName:"tr",align:"center"},"description"))),Object(l.b)("tbody",{parentName:"table"},Object(l.b)("tr",{parentName:"tbody"},Object(l.b)("td",{parentName:"tr",align:"center"},"split"),Object(l.b)("td",{parentName:"tr",align:"center"},"true"),Object(l.b)("td",{parentName:"tr",align:"center"},"true/false"),Object(l.b)("td",{parentName:"tr",align:"center"},"If false, Spark will not split pbf files, so parallelization will be per file.")))),Object(l.b)("p",null,"Ex. from the ",Object(l.b)("a",{parentName:"p",href:"#spark-shell"},"Spark Shell"),":"),Object(l.b)("pre",null,Object(l.b)("code",{parentName:"pre",className:"language-scala",metastring:'title="Scala"',title:'"Scala"'},'scala> val osmDF = spark.read.format("osm.pbf").option("split", "false").load("<osm files path here>")\n')),Object(l.b)("pre",null,Object(l.b)("code",{parentName:"pre",className:"language-python",metastring:'title="PySpark"',title:'"PySpark"'},'>>> osmDF = spark.read.format("osm.pbf").option("split", "false").load("<osm files path here>")\n')),Object(l.b)("pre",null,Object(l.b)("code",{parentName:"pre",className:"language-sql",metastring:'title="SQL"',title:'"SQL"'},"spark-sql> CREATE TABLE osm USING osm.pbf OPTIONS ( 'split' = 'false' ) LOCATION '<osm files path here>';\n")),Object(l.b)("h2",{id:"schema-definition"},"Schema definition"),Object(l.b)("p",null,"The Dataframe Schema used is the following one:"),Object(l.b)("pre",null,Object(l.b)("code",{parentName:"pre"},"root\n |-- id: long (nullable = true)\n |-- type: byte (nullable = true)\n |-- latitude: double (nullable = true)\n |-- longitude: double (nullable = true)\n |-- nodes: array (nullable = true)\n |    |-- element: long (containsNull = true)\n |-- relations: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- id: long (nullable = true)\n |    |    |-- relationType: byte (nullable = true)\n |    |    |-- role: string (nullable = true)\n |-- tags: map (nullable = true)\n |    |-- key: string\n |    |-- value: string (valueContainsNull = true)\n |-- info: struct (nullable = true)\n |    |-- version: integer (nullable = true)\n |    |-- timestamp: timestamp (nullable = true)\n |    |-- changeset: long (nullable = true)\n |    |-- userId: integer (nullable = true)\n |    |-- userName: string (nullable = true)\n |    |-- visible: boolean (nullable = true)\n\n")),Object(l.b)("p",null,"Where the column ",Object(l.b)("inlineCode",{parentName:"p"},"type")," could be:"),Object(l.b)("table",null,Object(l.b)("thead",{parentName:"table"},Object(l.b)("tr",{parentName:"thead"},Object(l.b)("th",{parentName:"tr",align:"center"},"value"),Object(l.b)("th",{parentName:"tr",align:"center"},"meaning"))),Object(l.b)("tbody",{parentName:"table"},Object(l.b)("tr",{parentName:"tbody"},Object(l.b)("td",{parentName:"tr",align:"center"},"0"),Object(l.b)("td",{parentName:"tr",align:"center"},"Node")),Object(l.b)("tr",{parentName:"tbody"},Object(l.b)("td",{parentName:"tr",align:"center"},"1"),Object(l.b)("td",{parentName:"tr",align:"center"},"Way")),Object(l.b)("tr",{parentName:"tbody"},Object(l.b)("td",{parentName:"tr",align:"center"},"2"),Object(l.b)("td",{parentName:"tr",align:"center"},"Relation")))),Object(l.b)("p",null,"Where the column ",Object(l.b)("inlineCode",{parentName:"p"},"relationType")," could be:"),Object(l.b)("table",null,Object(l.b)("thead",{parentName:"table"},Object(l.b)("tr",{parentName:"thead"},Object(l.b)("th",{parentName:"tr",align:"center"},"value"),Object(l.b)("th",{parentName:"tr",align:"center"},"meaning"))),Object(l.b)("tbody",{parentName:"table"},Object(l.b)("tr",{parentName:"tbody"},Object(l.b)("td",{parentName:"tr",align:"center"},"0"),Object(l.b)("td",{parentName:"tr",align:"center"},"Node")),Object(l.b)("tr",{parentName:"tbody"},Object(l.b)("td",{parentName:"tr",align:"center"},"1"),Object(l.b)("td",{parentName:"tr",align:"center"},"Way")),Object(l.b)("tr",{parentName:"tbody"},Object(l.b)("td",{parentName:"tr",align:"center"},"2"),Object(l.b)("td",{parentName:"tr",align:"center"},"Relation")),Object(l.b)("tr",{parentName:"tbody"},Object(l.b)("td",{parentName:"tr",align:"center"},"3"),Object(l.b)("td",{parentName:"tr",align:"center"},"Unrecognized")))),Object(l.b)("h2",{id:"all-in-one-jar"},"All in one jar"),Object(l.b)("p",null,"Usually, osm4scala is used from the ",Object(l.b)("a",{parentName:"p",href:"#spark-shell"},"Spark Shell")," or from a ",Object(l.b)("a",{parentName:"p",href:"#notebook"},"Notebook"),". For these cases,\nto simplify the way to add the connector as dependency, you have a shaded fat jar version with all dependencies that are\nnecessary.\nThe fat jar is near 5MB, so the size should be not a problem."),Object(l.b)("p",null,"As you probably know, Spark is base in Scala. Different Spark distributions are using different Scala versions.\nThis is the Spark/Scala version combination available for latest release v1.0.10:"),Object(l.b)("table",null,Object(l.b)("thead",{parentName:"table"},Object(l.b)("tr",{parentName:"thead"},Object(l.b)("th",{parentName:"tr",align:"center"},"Spark Branch"),Object(l.b)("th",{parentName:"tr",align:"center"},"Scala"),Object(l.b)("th",{parentName:"tr",align:"left"},"Packages"))),Object(l.b)("tbody",{parentName:"table"},Object(l.b)("tr",{parentName:"tbody"},Object(l.b)("td",{parentName:"tr",align:"center"},"2.4"),Object(l.b)("td",{parentName:"tr",align:"center"},"2.11"),Object(l.b)("td",{parentName:"tr",align:"left"},Object(l.b)("a",{parentName:"td",href:"https://search.maven.org/artifact/com.acervera.osm4scala/osm4scala-spark2-shaded_2.11/1.0.10/jar"},Object(l.b)("inlineCode",{parentName:"a"},"com.acervera.osm4scala:osm4scala-spark2-shaded_2.11:1.0.10")))),Object(l.b)("tr",{parentName:"tbody"},Object(l.b)("td",{parentName:"tr",align:"center"},"2.4"),Object(l.b)("td",{parentName:"tr",align:"center"},"2.12"),Object(l.b)("td",{parentName:"tr",align:"left"},Object(l.b)("a",{parentName:"td",href:"https://search.maven.org/artifact/com.acervera.osm4scala/osm4scala-spark2-shaded_2.12/1.0.10/jar"},Object(l.b)("inlineCode",{parentName:"a"},"com.acervera.osm4scala:osm4scala-spark2-shaded_2.12:1.0.10")))),Object(l.b)("tr",{parentName:"tbody"},Object(l.b)("td",{parentName:"tr",align:"center"},"3.0 / 3.1"),Object(l.b)("td",{parentName:"tr",align:"center"},"2.12"),Object(l.b)("td",{parentName:"tr",align:"left"},Object(l.b)("a",{parentName:"td",href:"https://search.maven.org/artifact/com.acervera.osm4scala/osm4scala-spark3-shaded_2.12/1.0.10/jar"},Object(l.b)("inlineCode",{parentName:"a"},"com.acervera.osm4scala:osm4scala-spark3-shaded_2.12:1.0.10")))))),Object(l.b)("p",null,"Although following sections are focus on Spark Shell and Notebooks, you can use the same technique in other situations where\nyou want to use the shaded version."),Object(l.b)("h3",{id:"why-a-fat-shaded-library"},"Why a fat shaded library?"),Object(l.b)("p",null,"Osm4scala has a transitive dependency with Java Google Protobuf library. Spark, Hadoop and other libraries in the\necosystem are using an old version of the same library (currently v2.5.0 from Mar, 2013) that is not compatible."),Object(l.b)("p",null,"To solve the conflict, I published the library in two fashion:"),Object(l.b)("ul",null,Object(l.b)("li",{parentName:"ul"},"Fat and Shaded as ",Object(l.b)("inlineCode",{parentName:"li"},"osm4scala-spark[2,3]-shaded")," that solves ",Object(l.b)("inlineCode",{parentName:"li"},"com.google.protobuf.**")," conflicts."),Object(l.b)("li",{parentName:"ul"},"Don't shaded as ",Object(l.b)("inlineCode",{parentName:"li"},"osm4scala-spark[2,3]"),", so you can solve the conflict on your way.")),Object(l.b)("h3",{id:"spark-shell"},"Spark Shell"),Object(l.b)("ol",null,Object(l.b)("li",{parentName:"ol"},Object(l.b)("p",{parentName:"li"},"Start the spark shell as usual, using the ",Object(l.b)("inlineCode",{parentName:"p"},"--packages")," option to add the right dependency. The dependency will depend to\nthe Spark Version that you are using. Please, check the reference table in the previous section."),Object(l.b)("pre",{parentName:"li"},Object(l.b)("code",{parentName:"pre",className:"language-shell",metastring:'title="Scala"',title:'"Scala"'},"bin/spark-shell --packages 'com.acervera.osm4scala:osm4scala-spark3-shaded_2.12:1.0.10'\n")),Object(l.b)("pre",{parentName:"li"},Object(l.b)("code",{parentName:"pre",className:"language-scala",metastring:'title="PySpark"',title:'"PySpark"'},"bin/pyspark --packages 'com.acervera.osm4scala:osm4scala-spark3-shaded_2.12:1.0.10'\n")),Object(l.b)("pre",{parentName:"li"},Object(l.b)("code",{parentName:"pre",className:"language-scala",metastring:'title="SQL"',title:'"SQL"'},"bin/spark-sql --packages 'com.acervera.osm4scala:osm4scala-spark3-shaded_2.12:1.0.10'\n"))),Object(l.b)("li",{parentName:"ol"},Object(l.b)("p",{parentName:"li"},"Create the Dataframe using the osm.pbf format, pointing to the pbf file or folder containing pbf files."),Object(l.b)("pre",{parentName:"li"},Object(l.b)("code",{parentName:"pre",className:"language-scala",metastring:'title="Scala"',title:'"Scala"'},'scala> val osmDF = spark.read.format("osm.pbf").load("<osm files path here>")\n')),Object(l.b)("pre",{parentName:"li"},Object(l.b)("code",{parentName:"pre",className:"language-python",metastring:'title="PySpark"',title:'"PySpark"'},'>>> osmDF = spark.read.format("osm.pbf").load("<osm files path here>")\n')),Object(l.b)("pre",{parentName:"li"},Object(l.b)("code",{parentName:"pre",className:"language-sql",metastring:'title="SQL"',title:'"SQL"'},"spark-sql> CREATE TABLE osm USING osm.pbf LOCATION '<osm files path here>';\n"))),Object(l.b)("li",{parentName:"ol"},Object(l.b)("p",{parentName:"li"},"Use the created dataframe as usual, keeping in mind the schema explained previously."),Object(l.b)("ul",{parentName:"li"},Object(l.b)("li",{parentName:"ul"},Object(l.b)("p",{parentName:"li"},"In the next example, we are going to count the number of different primitives in\nthe file. As explained in the schema, 0 are nodes, 1 ways and 2 relations."),Object(l.b)("pre",{parentName:"li"},Object(l.b)("code",{parentName:"pre",className:"language-scala",metastring:'title="Scala"',title:'"Scala"'},'scala> osmDF.groupBy("type").count().show()\n+----+--------+\n|type|   count|\n+----+--------+\n|   1| 2096455|\n|   2|   91971|\n|   0|19426617|\n+----+--------+\n')),Object(l.b)("pre",{parentName:"li"},Object(l.b)("code",{parentName:"pre",className:"language-python",metastring:'title="PySpark"',title:'"PySpark"'},'>>> osmDF.groupBy("type").count().show()\n+----+--------+\n|type|   count|\n+----+--------+\n|   1| 2096455|\n|   2|   91971|\n|   0|19426617|\n+----+--------+\n')),Object(l.b)("pre",{parentName:"li"},Object(l.b)("code",{parentName:"pre",className:"language-sql",metastring:'title="SQL"',title:'"SQL"'},"spark-sql> select type, count(type) from osm group by type\n1   338795\n2   10357\n0   2328075\n"))),Object(l.b)("li",{parentName:"ul"},Object(l.b)("p",{parentName:"li"},"In this other examples, we are going to extract all traffic lights as POIs."),Object(l.b)("pre",{parentName:"li"},Object(l.b)("code",{parentName:"pre",className:"language-scala",metastring:'title="Scala"',title:'"Scala"'},'scala> osmDF.select("latitude", "longitude", "tags").where("element_at(tags, \'highway\') == \'traffic_signals\'").show(10,false)\n+------------------+-------------------+------------------------------------------------------------------------------+\n|latitude          |longitude          |tags                                                                          |\n+------------------+-------------------+------------------------------------------------------------------------------+\n|54.59766649999997 |-5.8889806000000045|[highway -> traffic_signals]                                                  |\n|54.58006689999997 |-5.938683200000003 |[highway -> traffic_signals, traffic_signals -> signal]                       |\n|54.58260049999997 |-5.946187600000005 |[direction -> backward, highway -> traffic_signals, traffic_signals -> signal]|\n|51.90097769999996 |-8.470285700000005 |[highway -> traffic_signals]                                                  |\n|51.901616299999965|-8.470139700000004 |[highway -> traffic_signals]                                                  |\n|51.89978239999997 |-8.465829200000002 |[highway -> traffic_signals]                                                  |\n|51.89707529999997 |-8.474892800000001 |[highway -> traffic_signals]                                                  |\n|51.89784849999997 |-8.466895200000002 |[highway -> traffic_signals]                                                  |\n|51.89547809999997 |-8.476100900000002 |[highway -> traffic_signals]                                                  |\n|51.89772569999997 |-8.477145100000003 |[highway -> traffic_signals]                                                  |\n+------------------+-------------------+------------------------------------------------------------------------------+\nonly showing top 10 rows\n')),Object(l.b)("pre",{parentName:"li"},Object(l.b)("code",{parentName:"pre",className:"language-python",metastring:'title="PySpark"',title:'"PySpark"'},'>>> osmDF.select("latitude", "longitude", "tags").where("element_at(tags, \'highway\') == \'traffic_signals\'").show(10,False)\n+------------------+-------------------+------------------------------------------------------------------------------+\n|latitude          |longitude          |tags                                                                          |\n+------------------+-------------------+------------------------------------------------------------------------------+\n|54.59766649999997 |-5.8889806000000045|[highway -> traffic_signals]                                                  |\n|54.58006689999997 |-5.938683200000003 |[highway -> traffic_signals, traffic_signals -> signal]                       |\n|54.58260049999997 |-5.946187600000005 |[direction -> backward, highway -> traffic_signals, traffic_signals -> signal]|\n|51.90097769999996 |-8.470285700000005 |[highway -> traffic_signals]                                                  |\n|51.901616299999965|-8.470139700000004 |[highway -> traffic_signals]                                                  |\n|51.89978239999997 |-8.465829200000002 |[highway -> traffic_signals]                                                  |\n|51.89707529999997 |-8.474892800000001 |[highway -> traffic_signals]                                                  |\n|51.89784849999997 |-8.466895200000002 |[highway -> traffic_signals]                                                  |\n|51.89547809999997 |-8.476100900000002 |[highway -> traffic_signals]                                                  |\n|51.89772569999997 |-8.477145100000003 |[highway -> traffic_signals]                                                  |\n+------------------+-------------------+------------------------------------------------------------------------------+\nonly showing top 10 rows\n')),Object(l.b)("pre",{parentName:"li"},Object(l.b)("code",{parentName:"pre",className:"language-sql",metastring:'title="SQL"',title:'"SQL"'},'spark-sql> select latitude, longitude, tags from osm where type = 0 and element_at(tags, "highway") == \'traffic_signals\' limit 10;\n40.42125            -3.6844500000000004 {"crossing":"traffic_signals","crossing_ref":"zebra","highway":"traffic_signals"}\n40.41779000000001   -3.6241199999999996 {"highway":"traffic_signals"}\n40.41473000000003   -3.627109999999999  {"highway":"traffic_signals"}\n40.414200000000015  -3.6282099999999993 {"highway":"traffic_signals"}\n40.42635999999994   -3.727220000000005  {"crossing":"traffic_signals","highway":"traffic_signals"}\n40.41937999999995   -3.688820000000004  {"highway":"traffic_signals"}\n40.426489999999944  -3.687640000000004  {"highway":"traffic_signals","traffic_signals":"signal"}\n40.421339999999944  -3.683020000000004  {"highway":"traffic_signals"}\n40.41797999999994   -3.669340000000004  {"highway":"traffic_signals"}\n40.418319999999945  -3.6762200000000043 {"highway":"traffic_signals","traffic_signals":"signal"}\nTime taken: 0.128 seconds, Fetched 10 row(s)\n')))))),Object(l.b)("h3",{id:"notebook"},"Notebook"),Object(l.b)("p",null,"There are different notebooks solutions in the market and each one is using a different way to import libraries. But after\nimporting the library, you can use the osm4scala connector in the same way."),Object(l.b)("p",null,"For this section, we are going to use ",Object(l.b)("a",{parentName:"p",href:"https://jupyter.org"},"Jupyter Notebook")," and ",Object(l.b)("a",{parentName:"p",href:"https://jupyterlab.readthedocs.io/en/stable/"},"JupyterLab"),"."),Object(l.b)("div",{className:"admonition admonition-note alert alert--secondary"},Object(l.b)("div",{parentName:"div",className:"admonition-heading"},Object(l.b)("h5",{parentName:"div"},Object(l.b)("span",{parentName:"h5",className:"admonition-icon"},Object(l.b)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"},Object(l.b)("path",{parentName:"svg",fillRule:"evenodd",d:"M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"}))),"note")),Object(l.b)("div",{parentName:"div",className:"admonition-content"},Object(l.b)("p",{parentName:"div"},"If you can not access to a Jupyter Notebook\ninstallation, you can use ",Object(l.b)("a",{parentName:"p",href:"https://hub.docker.com/r/jupyter/all-spark-notebook"},"jupyter/all-spark-notebook")," Docker image as I will do.\n",Object(l.b)("a",{parentName:"p",href:"https://jupyter-docker-stacks.readthedocs.io/en/latest/index.html"},"Here"),", full documentation about how to install and use it."),Object(l.b)("p",{parentName:"div"},"To start the docker image, as easy as running the docker image and use the link provided:"),Object(l.b)("pre",{parentName:"div"},Object(l.b)("code",{parentName:"pre",className:"language-ssh"},"$ docker run -e JUPYTER_ENABLE_LAB=yes -d -p 8888:8888 -p 4040:4040 -p 4041:4041 jupyter/all-spark-notebook\n[I 11:02:45.132 NotebookApp] Serving notebooks from local directory: /home/jovyan\n[I 11:02:45.132 NotebookApp] Jupyter Notebook 6.2.0 is running at:\n[I 11:02:45.132 NotebookApp] http://479a92a85698:8888/?token=60ace2db5d456f7348c2ba0399cab986e36f4de9XX00a554\n[I 11:02:45.132 NotebookApp]  or http://127.0.0.1:8888/?token=60ace2db5d456f7348c2ba0399cab986e36f4de9XX00a554\n[I 11:02:45.132 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).\n[C 11:02:45.135 NotebookApp]\n\n    To access the notebook, open this file in a browser:\n        file:///home/jovyan/.local/share/jupyter/runtime/nbserver-6-open.html\n    Or copy and paste one of these URLs:\n        http://479a92a85698:8888/?token=60ace2db5d456f7348c2ba0399cab986e36f4de9XX00a554\n     or http://127.0.0.1:8888/?token=60ace2db5d456f7348c2ba0399cab986e36f4de9XX00a554\n\n")),Object(l.b)("p",{parentName:"div"},"If you prefer an online option, you can try ",Object(l.b)("a",{parentName:"p",href:"https://mybinder.org/v2/gh/jupyter/docker-stacks/master?filepath=README.ipynb"},"MyBinder"),"."))),Object(l.b)("ol",null,Object(l.b)("li",{parentName:"ol"},Object(l.b)("p",{parentName:"li"},"Create a new Notebook. For Scala, we are going to use the ",Object(l.b)("inlineCode",{parentName:"p"},"spylon-kernel"),".")),Object(l.b)("li",{parentName:"ol"},Object(l.b)("p",{parentName:"li"},"Add a new Cell and import the right osm4scala library for your Notebook installation, following the table at the start\nof the ",Object(l.b)("a",{parentName:"p",href:"#all-in-one-jar"},"All in one jar")," section. In or case, the version used is ",Object(l.b)("inlineCode",{parentName:"p"},"Spark v3.1.1")," with ",Object(l.b)("inlineCode",{parentName:"p"},"Scala 2.12"),"."),Object(l.b)("pre",{parentName:"li"},Object(l.b)("code",{parentName:"pre",className:"language-jypiter"},'%%init_spark\nlauncher.packages = ["com.acervera.osm4scala:osm4scala-spark3-shaded_2.12:1.0.10"]\n')),Object(l.b)("p",{parentName:"li"},"If you did not execute anything before, running the cell will start the Spark session. Sometime, depending to the\nNotebook used, ",Object(l.b)("strong",{parentName:"p"},"you will need to restart the Spark session (or Kernel session)"),".")),Object(l.b)("li",{parentName:"ol"},Object(l.b)("p",{parentName:"li"},"From the previous step, you can start creating dataframes from pbf files as we did before in previous sections."),Object(l.b)("p",{parentName:"li"},"Let's suppose that you uploaded a file called ",Object(l.b)("inlineCode",{parentName:"p"},"monaco-anonymized.osm.pbf")," into the notebook's ",Object(l.b)("inlineCode",{parentName:"p"},"work")," folder."),Object(l.b)("p",{parentName:"li"},"If you create a new Cell with next content, you will get all traffic signals in Monaco."),Object(l.b)("pre",{parentName:"li"},Object(l.b)("code",{parentName:"pre",className:"language-scala",metastring:'title="Scala"',title:'"Scala"'},'val osmDF = spark.read.format("osm.pbf").load("/home/jovyan/work/monaco-anonymized.osm.pbf")\nosmDF.select("latitude", "longitude")\n    .where("element_at(tags, \'highway\') == \'traffic_signals\'")\n    .show\n')),Object(l.b)("pre",{parentName:"li"},Object(l.b)("code",{parentName:"pre",className:"language-python",metastring:"{1}","{1}":!0},'%%python\nosm_df = spark.read.format("osm.pbf").load("/home/jovyan/work/monaco-anonymized.osm.pbf")\nosm_df.select("latitude", "longitude").where("element_at(tags, \'highway\') == \'traffic_signals\'").show()\n')),Object(l.b)("div",{parentName:"li",className:"admonition admonition-note alert alert--secondary"},Object(l.b)("div",{parentName:"div",className:"admonition-heading"},Object(l.b)("h5",{parentName:"div"},Object(l.b)("span",{parentName:"h5",className:"admonition-icon"},Object(l.b)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"},Object(l.b)("path",{parentName:"svg",fillRule:"evenodd",d:"M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"}))),"note")),Object(l.b)("div",{parentName:"div",className:"admonition-content"},Object(l.b)("p",{parentName:"div"},"Pay attention to the first line in the Python Cell. Because the kernel used is using ",Object(l.b)("inlineCode",{parentName:"p"},"Scala")," as default, you need to add\nthe ",Object(l.b)("strong",{parentName:"p"},Object(l.b)("inlineCode",{parentName:"strong"},"%%python"))," header."))),Object(l.b)("p",{parentName:"li"},"Next, a screenshot with the output generated:\n",Object(l.b)("img",{alt:"Notebook Screenshoot",src:t(129).default})),Object(l.b)("p",{parentName:"li"},"Of course, from the dataframe you can create beautiful maps, graphs, etc. But that is out of the scope of this documentation."))),Object(l.b)("h3",{id:"spark-application"},"Spark application"),Object(l.b)("p",null,"When we need to write more complex analysis, data extractions, ETLs, etc, it is necessary to write Spark applications."),Object(l.b)("ol",null,Object(l.b)("li",{parentName:"ol"},Object(l.b)("p",{parentName:"li"},"Import the spark connector ",Object(l.b)("strong",{parentName:"p"},"it is not really necessary")," because the integration is transparent."),Object(l.b)("p",{parentName:"li"},"Only two ",Object(l.b)("em",{parentName:"p"},"possible")," advantages (not available if using Python) are:"),Object(l.b)("ul",{parentName:"li"},Object(l.b)("li",{parentName:"ul"},Object(l.b)("p",{parentName:"li"},"The use of static constants, for example, to avoid ",Object(l.b)("inlineCode",{parentName:"p"},"magic numbers")," for primitive and relation types.")),Object(l.b)("li",{parentName:"ul"},Object(l.b)("p",{parentName:"li"},"Using the library as part of Unit Testing or Integration Testing.")),Object(l.b)("li",{parentName:"ul"},Object(l.b)("p",{parentName:"li"},"Adding osm4scala jar library as part of the deployable artifact."),Object(l.b)("p",{parentName:"li"},Object(l.b)("strong",{parentName:"p"},"For Python"),", like in Scala, it is not necessary to import the library except in runtime. But unlike in Scala, you\ncan not ",Object(l.b)("em",{parentName:"p"},"easily")," to import and use facilities from the Scala library. So in this case, you can jump to the next step."),Object(l.b)("pre",{parentName:"li"},Object(l.b)("code",{parentName:"pre",className:"language-sbt",metastring:'title="Sbt"',title:'"Sbt"'},'libraryDependencies += "com.acervera.osm4scala" % "osm4scala-spark3-shaded_2.12" % "1.0.10"\n')),Object(l.b)("pre",{parentName:"li"},Object(l.b)("code",{parentName:"pre",className:"language-xml",metastring:'title="Maven"',title:'"Maven"'},"<dependency>\n    <groupId>com.acervera.osm4scala</groupId>\n    <artifactId>osm4scala-spark3-shaded_2.12</artifactId>\n    <version>1.0.10</version>\n</dependency>\n")),Object(l.b)("div",{parentName:"li",className:"admonition admonition-tip alert alert--success"},Object(l.b)("div",{parentName:"div",className:"admonition-heading"},Object(l.b)("h5",{parentName:"div"},Object(l.b)("span",{parentName:"h5",className:"admonition-icon"},Object(l.b)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"12",height:"16",viewBox:"0 0 12 16"},Object(l.b)("path",{parentName:"svg",fillRule:"evenodd",d:"M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"}))),"Reduce artifact size.")),Object(l.b)("div",{parentName:"div",className:"admonition-content"},Object(l.b)("p",{parentName:"div"},"The shaded dependency is near 5MB. You can add this dependency as a package when you submit the job instead to include it\nin the deployable artifact generated. To do it, set the scope dependency as ",Object(l.b)("inlineCode",{parentName:"p"},"Test")," or ",Object(l.b)("inlineCode",{parentName:"p"},"Provided"),"."),Object(l.b)("p",{parentName:"div"},Object(l.b)("em",{parentName:"p"},"If you don't know what I'm talking about, don't pay too much attention and forget it.")," \ud83d\ude09")))))),Object(l.b)("li",{parentName:"ol"},Object(l.b)("p",{parentName:"li"},"Create the Dataframe using the osm.pbf format, pointing to the pbf file or folder containing pbf files, and use as usual."),Object(l.b)("pre",{parentName:"li"},Object(l.b)("code",{parentName:"pre",className:"language-scala",metastring:'title="Scala / PrimitivesCounter.scala"',title:'"Scala',"/":!0,'PrimitivesCounter.scala"':!0},'\nimport com.acervera.osm4scala.spark.OsmSqlEntity\nimport org.apache.spark.sql.SparkSession\n\nobject PrimitivesCounter {\n\n  def main(args: Array[String]): Unit = {\n    val spark = SparkSession\n      .builder()\n      .appName("Primitives counter")\n      .getOrCreate()\n\n    spark.read\n      .format("osm.pbf")\n      .load(args(0))\n      .groupBy(OsmSqlEntity.FIELD_TYPE)\n      .count\n      .show\n  }\n}\n\n')),Object(l.b)("pre",{parentName:"li"},Object(l.b)("code",{parentName:"pre",className:"language-python",metastring:'title="PySpark / PrimiriveCounter.py"',title:'"PySpark',"/":!0,'PrimiriveCounter.py"':!0},'from pyspark.sql import SparkSession\nimport sys\n\nif __name__ == \'__main__\':\n\n    spark = SparkSession.builder.appName("Primitives counter").getOrCreate()\n\n    spark.read.format("osm.pbf")\\\n        .load(sys.argv[1])\\\n        .groupBy("type")\\\n        .count()\\\n        .show()\n'))),Object(l.b)("li",{parentName:"ol"},Object(l.b)("p",{parentName:"li"},"Submit the application to your Spark cluster."),Object(l.b)("pre",{parentName:"li"},Object(l.b)("code",{parentName:"pre",className:"language-shell",metastring:'title="Scala"',title:'"Scala"'},"bin/spark-submit \\\n    --packages 'com.acervera.osm4scala:osm4scala-spark3-shaded_2.12:1.0.10' \\\n    examples/spark-documentation/target/scala-2.12/osm4scala-examples-spark-documentation_2.12-1.0.10-SNAPSHOT.jar \\\n    /tmp/osm/monaco-anonymized.osm.pbf\n")),Object(l.b)("pre",{parentName:"li"},Object(l.b)("code",{parentName:"pre",className:"language-shell",metastring:'title="PySpark"',title:'"PySpark"'},"bin/spark-submit \\\n    --packages 'com.acervera.osm4scala:osm4scala-spark3-shaded_2.12:1.0.10' \\\n    examples/spark-documentation/src/main/scala/com/acervera/osm4scala/examples/spark/documentation/PrimiriveCounter.py \\\n    /tmp/osm/monaco-anonymized.osm.pbf\n")),Object(l.b)("div",{parentName:"li",className:"admonition admonition-note alert alert--secondary"},Object(l.b)("div",{parentName:"div",className:"admonition-heading"},Object(l.b)("h5",{parentName:"div"},Object(l.b)("span",{parentName:"h5",className:"admonition-icon"},Object(l.b)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"},Object(l.b)("path",{parentName:"svg",fillRule:"evenodd",d:"M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"}))),"Optional --packages.")),Object(l.b)("div",{parentName:"div",className:"admonition-content"},Object(l.b)("p",{parentName:"div"},"You will not need to add ",Object(l.b)("inlineCode",{parentName:"p"},"--packages 'com.acervera.osm4scala:osm4scala-spark3-shaded_2.12:1.0.10'")," if it is part of the\ndeployed artifact."))))),Object(l.b)("h3",{id:"more-examples"},"More Examples"),Object(l.b)("p",null,"Following, more examples. This time, we will create a SQL temporal view and SQL:"),Object(l.b)("pre",null,Object(l.b)("code",{parentName:"pre",className:"language-scala"},'scala> val osmDF = spark.sqlContext.read.format("osm.pbf").load("<osm files path here>")\nosmDF: org.apache.spark.sql.DataFrame = [id: bigint, type: tinyint ... 5 more fields]\nscala> osmDF.createOrReplaceTempView("osm")\n')),Object(l.b)("pre",null,Object(l.b)("code",{parentName:"pre",className:"language-scala",metastring:'title="Primitives counter"',title:'"Primitives','counter"':!0},'scala> spark.sql("select type, count(*) as num_primitives from osm group by type").show()\n+----+--------------+\n|type|num_primitives|\n+----+--------------+\n|   1|        338795|\n|   2|         10357|\n|   0|       2328075|\n+----+--------------+\n')),Object(l.b)("pre",null,Object(l.b)("code",{parentName:"pre",className:"language-scala",metastring:'title="Extract all keys used in tags"',title:'"Extract',all:!0,keys:!0,used:!0,in:!0,'tags"':!0},'scala> spark.sql("select distinct(explode(map_keys(tags))) as tag_key from osm order by tag_key asc").show()\n+------------------+\n|           tag_key|\n+------------------+\n|             Calle|\n|        Conference|\n|             Exper|\n|             FIXME|\n|         ISO3166-1|\n|  ISO3166-1:alpha2|\n|  ISO3166-1:alpha3|\n| ISO3166-1:numeric|\n|         ISO3166-2|\n|           MAC_dec|\n|            Nombre|\n|            Numero|\n|              Open|\n|        Peluqueria|\n|    Residencia UEM|\n|          Telefono|\n|         abandoned|\n| abandoned:amenity|\n| abandoned:barrier|\n|abandoned:building|\n+------------------+\nonly showing top 20 rows\n')),Object(l.b)("pre",null,Object(l.b)("code",{parentName:"pre",className:"language-scala",metastring:'title="Extract id, coords and tags from all nodes"',title:'"Extract',"id,":!0,coords:!0,and:!0,tags:!0,from:!0,all:!0,'nodes"':!0},'scala> spark.sql("select id, latitude, longitude, tags from osm where type = 0").show()\n+--------+------------------+-------------------+--------------------+\n|      id|          latitude|          longitude|                tags|\n+--------+------------------+-------------------+--------------------+\n|  171933|          40.42006|-3.7016600000000004|                  []|\n|  171946|          40.42125|-3.6844500000000004|[highway -> traff...|\n|  171948|40.420230000000004|-3.6877900000000006|                  []|\n|  171951|40.417350000000006|-3.6889800000000004|                  []|\n|  171952|          40.41499|-3.6889800000000004|                  []|\n|  171953|          40.41277|-3.6889000000000003|                  []|\n|  171954|          40.40946|-3.6887900000000005|                  []|\n|  171959|          40.40326|-3.7012200000000006|                  []|\n|20952874|          40.42099|-3.6019200000000007|                  []|\n|20952875|40.422610000000006|-3.5994900000000007|                  []|\n|20952878| 40.42136000000001| -3.601470000000001|                  []|\n|20952879| 40.42262000000001| -3.599770000000001|                  []|\n|20952881| 40.42905000000001|-3.5970500000000007|                  []|\n|20952883| 40.43131000000001|-3.5961000000000007|                  []|\n|20952888| 40.42930000000001| -3.596590000000001|                  []|\n|20952890| 40.43012000000001|-3.5961500000000006|                  []|\n|20952891| 40.43043000000001|-3.5963600000000007|                  []|\n|20952892| 40.43057000000001|-3.5969100000000007|                  []|\n|20952893| 40.43039000000001|-3.5973200000000007|                  []|\n|20952895| 40.42967000000001|-3.5972300000000006|                  []|\n+--------+------------------+-------------------+--------------------+\nonly showing top 20 rows\n')),Object(l.b)("pre",null,Object(l.b)("code",{parentName:"pre",className:"language-scala",metastring:'title="Extract id, nodes and tags from all ways"',title:'"Extract',"id,":!0,nodes:!0,and:!0,tags:!0,from:!0,all:!0,'ways"':!0},'scala> spark.sql("select id, nodes, tags from osm where type = 1").show()\n+-------+--------------------+--------------------+\n|     id|               nodes|                tags|\n+-------+--------------------+--------------------+\n|3996189|[23002322, 230022...|[name -> M-40, in...|\n|3996190|[20952892, 213645...|[name -> Avenida ...|\n|3996191|[21364526, 253693...|[lanes -> 2, onew...|\n|3996192|[20952914, 242495...|[name -> Plaza de...|\n|3996195|[20952923, 421448...|[name -> Calle de...|\n|3996196|[20952942, 209529...|[name -> Avenida ...|\n|3996197|[20952893, 209628...|[name -> Avenida ...|\n|3996199|[20952929, 209529...|[name -> Calle de...|\n|3996203|[20952948, 391553...|[name -> Calle de...|\n|3997425|[20960686, 219912...|[name -> Avenida ...|\n|3997426|[2424952617, 2095...|[name -> Avenida ...|\n|3997427|[20960717, 209606...|[name -> Calle de...|\n|3997428|[20960693, 209607...|[highway -> terti...|\n|3997429|[20960696, 421448...|[name -> Calle de...|\n|3997430|[20963025, 209630...|[name -> Paseo de...|\n|3997432|[20960688, 209607...|[name -> Calle de...|\n|3997433|[1811010970, 1811...|[name -> Calle de...|\n|4004278|[255148257, 21067...|[name -> Calle de...|\n|4004280|[20963101, 209630...|[name -> Calle de...|\n|4004281|[25530614, 297977...|[name -> Calle de...|\n+-------+--------------------+--------------------+\nonly showing top 20 rows\n')),Object(l.b)("pre",null,Object(l.b)("code",{parentName:"pre",className:"language-scala",metastring:'title="Extract id, relations and tags from all relations"',title:'"Extract',"id,":!0,relations:!0,and:!0,tags:!0,from:!0,all:!0,'relations"':!0},'scala> spark.sql("select id, relations, tags from osm where type = 2").show()\n+-----+--------------------+--------------------+\n|   id|           relations|                tags|\n+-----+--------------------+--------------------+\n|11331|[[2609596233, 0, ...|[network -> Cerca...|\n|11332|[[196618381, 1, p...|[network -> Cerca...|\n|14612|[[24698019, 1, ou...|[website -> http:...|\n|30117|[[26629303, 1, ou...|[type -> multipol...|\n|30399|[[307006515, 1, i...|[website -> http:...|\n|38757|[[6120746, 1, ], ...|[network -> lcn, ...|\n|38965|[[44571128, 1, fr...|[type -> restrict...|\n|48292|[[317775809, 0, s...|[network -> Metro...|\n|49958|[[308868559, 0, v...|[type -> restrict...|\n|49959|[[308868558, 0, v...|[type -> restrict...|\n|50874|[[26141446, 1, ou...|[name -> Escuela ...|\n|52312|[[24531942, 1, ou...|[name -> Pista pr...|\n|52313|[[24698560, 1, ou...|[type -> multipol...|\n|53157|[[2609596077, 0, ...|[network -> Cerca...|\n|55085|[[246285922, 0, s...|[network -> Cerca...|\n|55087|[[194005015, 1, ]...|[network -> Cerca...|\n|55799|[[28775036, 1, ou...|[type -> multipol...|\n|56044|[[258556530, 0, s...|[network -> Metro...|\n|56260|[[144383571, 1, o...|[name -> Ayuntami...|\n|56791|[[32218973, 0, st...|[network -> Metro...|\n+-----+--------------------+--------------------+\nonly showing top 20 rows\n\n')),Object(l.b)("pre",null,Object(l.b)("code",{parentName:"pre",className:"language-scala",metastring:'title="Extract id, relations and tags from all relations"',title:'"Extract',"id,":!0,relations:!0,and:!0,tags:!0,from:!0,all:!0,'relations"':!0},'scala> spark.sql("select id, type, info.version, info.userId, info.userName, date_format(info.timestamp, \\"dd-MMM-y kk:mm:ss z\\") as timestamp from osm where info.userId IS NOT NULL").show(5, false)\n\n+---------+----+-------+--------+------------+------------------------+\n|id       |type|version|userId  |userName    |timestamp               |\n+---------+----+-------+--------+------------+------------------------+\n|10966459 |2   |26     |18XXXX  |XXXXX       |01-Oct-2020 18:44:49 IST|\n|166399497|1   |4      |16XXXXX |XXXXXXXXXXXX|09-Aug-2019 05:52:21 IST|\n|434583789|1   |7      |25XXXX  |XXXXXX      |05-Mar-2020 06:48:01 IST|\n|161752645|1   |14     |11XXXXXX|XXXXXXXXXXX |22-Mar-2021 09:08:10 IST|\n|690021772|1   |1      |31XXXX  |XXXXX       |14-May-2019 19:18:06 IST|\n+---------+----+-------+--------+------------+------------------------+\nonly showing top 5 rows\n')),Object(l.b)("h2",{id:"plain-non-shaded-jar-dependency"},"Plain (non-shaded jar) dependency."),Object(l.b)("p",null,"Sometimes we need to write more complex applications, analysis, data extractions, ETLs, integrate with other libraries,\nunit testing, etc.\nIn that case, the best practice is to manage dependencies using ",Object(l.b)("inlineCode",{parentName:"p"},"sbt")," or ",Object(l.b)("inlineCode",{parentName:"p"},"maven"),", instead to import the shaded file."),Object(l.b)("p",null,"OSM Pbf files are based on ",Object(l.b)("a",{parentName:"p",href:"https://developers.google.com/protocol-buffers"},"Protocol Buffer"),", so ",Object(l.b)("a",{parentName:"p",href:"https://scalapb.github.io"},"Scalapb")," is\nused as deserializer so it's the unique transitive dependency."),Object(l.b)("p",null,"This is the Spark/Scala version combination available for latest release v1.0.10:"),Object(l.b)("table",null,Object(l.b)("thead",{parentName:"table"},Object(l.b)("tr",{parentName:"thead"},Object(l.b)("th",{parentName:"tr",align:"center"},"Spark branch"),Object(l.b)("th",{parentName:"tr",align:"center"},"Scalapb"),Object(l.b)("th",{parentName:"tr",align:"center"},"Scala"),Object(l.b)("th",{parentName:"tr",align:"left"},"Packages"))),Object(l.b)("tbody",{parentName:"table"},Object(l.b)("tr",{parentName:"tbody"},Object(l.b)("td",{parentName:"tr",align:"center"},"2.4"),Object(l.b)("td",{parentName:"tr",align:"center"},"0.9.7"),Object(l.b)("td",{parentName:"tr",align:"center"},"2.11"),Object(l.b)("td",{parentName:"tr",align:"left"},Object(l.b)("a",{parentName:"td",href:"https://search.maven.org/artifact/com.acervera.osm4scala/osm4scala-spark2_2.11/1.0.10/jar"},Object(l.b)("inlineCode",{parentName:"a"},"com.acervera.osm4scala:osm4scala-spark2_2.11:1.0.10")))),Object(l.b)("tr",{parentName:"tbody"},Object(l.b)("td",{parentName:"tr",align:"center"},"2.4"),Object(l.b)("td",{parentName:"tr",align:"center"},"0.10.2"),Object(l.b)("td",{parentName:"tr",align:"center"},"2.12"),Object(l.b)("td",{parentName:"tr",align:"left"},Object(l.b)("a",{parentName:"td",href:"https://search.maven.org/artifact/com.acervera.osm4scala/osm4scala-spark2_2.12/1.0.10/jar"},Object(l.b)("inlineCode",{parentName:"a"},"com.acervera.osm4scala:osm4scala-spark2_2.12:1.0.10")))),Object(l.b)("tr",{parentName:"tbody"},Object(l.b)("td",{parentName:"tr",align:"center"},"3.0 / 3.1"),Object(l.b)("td",{parentName:"tr",align:"center"},"0.10.2"),Object(l.b)("td",{parentName:"tr",align:"center"},"2.12"),Object(l.b)("td",{parentName:"tr",align:"left"},Object(l.b)("a",{parentName:"td",href:"https://search.maven.org/artifact/com.acervera.osm4scala/osm4scala-spark3_2.12/1.0.10/jar"},Object(l.b)("inlineCode",{parentName:"a"},"com.acervera.osm4scala:osm4scala-spark3_2.12:1.0.10")))))),Object(l.b)("p",null,"After importing the connector, you can use it as we explained in the ",Object(l.b)("a",{parentName:"p",href:"#all-in-one-jar"},"All in one section"),". So lets see\nhow to import the library in our project and few examples."),Object(l.b)("h3",{id:"resolving-dependency-conflicts"},"Resolving dependency conflicts"),Object(l.b)("p",null,"Osm4scala has a transitive dependency with Java Google Protobuf library.\nSpark, Hadoop and other libraries in the ecosystem are using an older version of the same library (currently v2.5.0 from Mar, 2013) that is not compatible."),Object(l.b)("p",null,"To be able to resolve this conflicts, you will need to ",Object(l.b)("inlineCode",{parentName:"p"},"shade")," your deployed jar. The conflict comes from the package ",Object(l.b)("inlineCode",{parentName:"p"},"com.google.protobuf"),"."),Object(l.b)("p",null,"Following, how to do it using SBT:"),Object(l.b)("pre",null,Object(l.b)("code",{parentName:"pre",className:"language-scala",metastring:'title="Sbt"',title:'"Sbt"'},'assemblyShadeRules in assembly := Seq(\nShadeRule\n  .rename("com.google.protobuf.**" -> "shadeproto.@1")\n  .inAll\n)\n')),Object(l.b)("p",null,"It is possible to do the same using the ",Object(l.b)("a",{parentName:"p",href:"https://maven.apache.org/plugins/maven-shade-plugin/index.html"},"shade maven plugin"),"."))}p.isMDXComponent=!0}}]);